from datetime import timedelta
from datetime import datetime
import pandas as pd
from os import remove
from catboost import CatBoostRegressor as cbr
import joblib

from airflow import DAG
from airflow.operators.python import PythonOperator

from airflow.providers.amazon.aws.hooks.s3 import S3Hook

import logging
from news_parser import Scrapper
from Parsing_index import IMOEXDataDownloader, SPBEDataDownloader
from airflow.utils.dates import days_ago
from botocore.exceptions import (
    ConnectTimeoutError,
    EndpointConnectionError,
    ConnectionError,
)

_LOG = logging.getLogger()
_LOG.addHandler(logging.StreamHandler())

cid = "s3_connection"
s3_hook = S3Hook(cid)


DEFAULT_ARGS = {
    "owner": "Team 22",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 3,
    "retry_delay": timedelta(minutes=1)
}

dag = DAG(
    "main_dag",
    tags=["mlops"],
    catchup=False,
    start_date=days_ago(2),
    default_args=DEFAULT_ARGS,
    schedule_interval="@once",
)

def init():
    """
    Step0: Pipeline initialisation.
    """
    info = {}
    info["start_timestamp"] = datetime.now().strftime("%Y%m%d %H:%M")
    # Импользуем данные с сегодня на 2 года назад.
    info["date_start"] = datetime.now().strftime("%Y-%m-%d")
    info["date_end"] = datetime.now().strftime("%Y-%m-%d")
    return info


def scrape_news(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="init")
    info["data_download_start"] = datetime.now().strftime("%Y%m%d %H:%M")
    start_date = info["date_start"]
    end_date = info["date_end"]

    scr = Scrapper("https://www.finam.ru/publications/section/market/date/", start_date, end_date)
    scr.parse()
    news_df = scr.estimate()

    s3_hook.download_file(
        key = "finam_news_scored.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    s3_news_df = pd.read_csv("data/finam_news_scored.csv")
    news_df = pd.concat([s3_news_df,news_df], ignore_index=True)
    news_df.to_csv('news.csv', index=False)
    s3_hook.load_file('news.csv', key = "finam_news_scored.csv", 
                          bucket_name="studcamp-ml", replace=True)
    remove('data/finam_news_scored.csv')
    
    _LOG.info("News loading finished.")

    info["data_download_end"] = datetime.now().strftime("%Y%m%d %H:%M")

    return info

def scrape_index(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="init")
    info["index_download_start"] = datetime.now().strftime("%Y%m%d %H:%M")
    start_date = info["date_start"]
    end_date = info["date_end"]

    imoex_pars = IMOEXDataDownloader(start_date= start_date, end_date= end_date)
    spbe_pars = SPBEDataDownloader(start_date= start_date, end_date= end_date)

    imoex_pars.get_history_data()
    imoex_df = imoex_pars.main_df

    spbe_pars.get_history_data()
    spbe_df = spbe_pars.main_df

    s3_hook.download_file(
        key="IMOEX_filled.csv",
        bucket_name="studcamp-ml",
        local_path="data",
        preserve_file_name=True,
        use_autogenerated_subdir=False
    )

    s3_hook.download_file(
        key="SPBIRUS2_filled.csv",
        bucket_name="studcamp-ml",
        local_path="data",
        preserve_file_name=True,
        use_autogenerated_subdir=False
    )

    s3_hook.load_file("IMOEX_filled.csv", key = "IMOEX_filled.csv", 
                          bucket_name="studcamp-ml", replace=True)
    remove('IMOEX_filled.csv')
    
    
    _LOG.info("IMOEX index loading finished.")
    
    s3_hook.load_file("SPBIRUS2_filled.csv", key = "SPBIRUS2_filled.csv",
                          bucket_name="studcamp-ml", replace=True)
    remove('SPBIRUS2_filled.csv')
    
    _LOG.info("SPBE index loading finished.")
    

    info["index_download_end"] = datetime.now().strftime("%Y%m%d %H:%M")

    return info


def query_forming_imoex(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids=["scrape_news", "scrape_index"])
    info["query_imoex_forming_start"] = datetime.now().strftime("%Y%m%d %H:%M")

    max_news_count = 20
    lookback = 65

    s3_hook.download_file(
        key = "finam_news_scored.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    news = pd.read_csv("data/finam_news_scored.csv")

    s3_hook.download_file(
        key = "IMOEX_filled.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    df_filled = pd.read_csv("data/IMOEX_filled.csv")

    news["day"] = pd.to_datetime(news["day"])
    cur_date = news.iloc[-1].day

    query = pd.DataFrame()
    news_count = news[news.day == cur_date].shape[0]
    query["news_size"] = [news_count]
    next_date = cur_date+timedelta(1)
    query["year"] = [next_date.year]
    query["month"] = [next_date.month]
    query["day"] = [next_date.day]
    remove('finam_news_scored.csv')
    remove('data/IMOEX_filled.csv')

    res = news[news.day == cur_date].sample(min(max_news_count, news_count)).score

    for i in range(max_news_count - min(max_news_count, news_count)):
        res = res.append(pd.Series([0.0]), ignore_index=True)

    res = res.reset_index(drop=True)

    for i in range(max_news_count):
        query[f"news{i}"] = res[i]

    lags = df_filled[df_filled.date.between(str(cur_date-timedelta(days=lookback)),str(cur_date))].open.reset_index(drop=True)
    
    for i in range(1,lookback+1):
        query["shift"+str(i)] = lags[lags.size-i]
    
    info["query_imoex"] = query
    _LOG.info("Query forming finished.")
    info["query_imoex_forming_end"] = datetime.now().strftime("%Y%m%d %H:%M")

    return info

def query_forming_spbi(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids=["scrape_news", "scrape_index"])
    info["query_spbi_forming_start"] = datetime.now().strftime("%Y%m%d %H:%M")

    max_news_count = 20
    lookback = 65

    s3_hook.download_file(
        key = "finam_news_scored.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    news = pd.read_csv("data/finam_news_scored.csv")

    s3_hook.download_file(
        key = "SPBIRUS2_filled.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    df_filled = pd.read_csv("data/SPBIRUS2_filled.csv")

    news["day"] = pd.to_datetime(news["day"])
    cur_date = news.iloc[-1].day

    query = pd.DataFrame()
    news_count = news[news.day == cur_date].shape[0]
    query["news_size"] = [news_count]
    next_date = cur_date+timedelta(1)
    query["year"] = [next_date.year]
    query["month"] = [next_date.month]
    query["day"] = [next_date.day]
    remove('finam_news_scored.csv')
    remove('data/SPBIRUS2_filled.csv')

    res = news[news.day == cur_date].sample(min(max_news_count, news_count)).score

    for i in range(max_news_count - min(max_news_count, news_count)):
        res = res.append(pd.Series([0.0]), ignore_index=True)

    res = res.reset_index(drop=True)

    for i in range(max_news_count):
        query[f"news{i}"] = res[i]

    lags = df_filled[df_filled.date.between(str(cur_date-timedelta(days=lookback)),str(cur_date))].open.reset_index(drop=True)
    
    for i in range(1,lookback+1):
        query["shift"+str(i)] = lags[lags.size-i]
    
    info["query_spbi"] = query
    _LOG.info("Query forming finished.")
    info["query_spbi_forming_end"] = datetime.now().strftime("%Y%m%d %H:%M")

    return info

def model_predict_imoex(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="query_forming_imoex")
    info["prediction_imoex_start"] = datetime.now().strftime("%Y%m%d %H:%M")

    s3_hook.download_file(
        key = "catboost_imoex.joblib",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    model = joblib.load("data/catboost_imoex.joblib")
    query = info["query_imoex"]
    preds = model.predict(query)
    preds = pd.DataFrame({'date':info["date_start"], 'open': preds})
    params = model.get_all_params()
    info["cb_params_imoex"] = params
    preds.to_csv('data/catboost_preds_imoex.csv', index = False)

    s3_hook.load_file('data/catboost_preds_imoex.csv', key = "prediction_imoex.csv", 
                        bucket_name="studcamp-ml", replace=True)
    remove('data/catboost.joblib')
    remove('data/catboost_preds_imoex.csv')
    info["predict_imoex"] = preds
    _LOG.info("Prediction_imoex_finished.")
    info["prediction_imoex_end"] = datetime.now().strftime("%Y%m%d %H:%M")
    return info
    
def model_predict_spbi(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="query_forming_spbi")
    info["prediction_spbi_start"] = datetime.now().strftime("%Y%m%d %H:%M")

    s3_hook.download_file(
        key = "catboost_spbi.joblib",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    model = joblib.load("data/catboost_spbi.joblib")
    query = info["query_spbi"]
    preds = model.predict(query)
    preds = pd.DataFrame({'date':info["date_start"], 'open': preds})
    params = model.get_all_params()
    info["cb_params_spbi"] = params
    preds.to_csv('data/catboost_preds_spbi.csv', index = False)

    s3_hook.load_file('data/catboost_preds_spbi.csv', key = "prediction_spbi.csv", 
                        bucket_name="studcamp-ml", replace=True)
    remove('data/catboost_spbi.joblib')
    remove('data/catboost_preds_spbi.csv')
    info["predict_spbi"] = preds
    _LOG.info("Prediction_spbi_finished.")
    info["prediction_spbi_end"] = datetime.now().strftime("%Y%m%d %H:%M")
    return info

def retrain_model_imoex(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="prediction_imoex")
    info["retraining_imoex_start"] = datetime.now().strftime("%Y%m%d %H:%M") 
    params = info["cb_params_imoex"]

    s3_hook.download_file(
        key = "for_catboost_imoex.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )

    df = pd.read_csv('data/for_catboost_imoex.csv')
    s3_hook.download_file(
        key = "yesterday_imoex_query.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )

    q = pd.read_csv('data/yesterday_imoex_query.csv')
    df = pd.concat([df, q], reset_index = False)
    cbr = cbr(**params)
    cbr.fit(df)
    joblib.dump(cbr, "data/catboost_imoex.joblib")
    s3_hook.load_file("data/catboost_imoex.joblib", key = "catboost_imoex.joblib", 
                        bucket_name="studcamp-ml", replace=True)
    remove('data/catboost_imoex.joblib')

    today_query = info["query_imoex"]
    today_query.to_csv("data/today_query_spbi")
    s3_hook.load_file("data/today_query_spbi", key = "yesterday_imoex_query.csv", 
                        bucket_name="studcamp-ml", replace=True)
    remove('data/for_catboost.csv')
    remove('data/yesterday_imoex_query.csv')

    _LOG.info("Retrain IMOEX finished.")
    info["retraining_imoex_end"] = datetime.now().strftime("%Y%m%d %H:%M")
    return info

def retrain_model_spbi(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="prediction_spbi")
    info["retraining_spbi_start"] = datetime.now().strftime("%Y%m%d %H:%M") 
    params = info["cb_params_spbi"]

    s3_hook.download_file(
        key = "for_catboost_spbi.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )

    df = pd.read_csv('data/for_catboost_spbi.csv')
    s3_hook.download_file(
        key = "yesterday_spbi_query.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )

    q = pd.read_csv('data/yesterday_spbi_query.csv')
    df = pd.concat([df, q], reset_index = False)
    cbr = cbr(**params)
    cbr.fit(df)
    joblib.dump(cbr, "data/catboost_spbi.joblib")
    s3_hook.load_file("data/catboost_spbi.joblib", key = "catboost_spbi.joblib", 
                        bucket_name="studcamp-ml", replace=True)
    remove('data/catboost_spbi.joblib')

    today_query = info["query_spbi"]
    today_query.to_csv("data/today_query_spbi,csv")
    s3_hook.load_file("data/today_query,csv", key = "yesterday_query.csv", 
                        bucket_name="studcamp-ml", replace=True)
    remove('data/for_catboost.csv')
    remove('data/yesterday_query.csv')
    remove('data/today_query.csv')

    _LOG.info("Retrain spbi finished.")
    info["retraining_spbi_end"] = datetime.now().strftime("%Y%m%d %H:%M")
    return info


def save_last_hundred_imoex(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="retrain_imoex")
    info["save_last_imoex_start"] = datetime.now().strftime("%Y%m%d %H:%M")   

    s3_hook.download_file(
        key = "IMOEX_filled.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    df_filled = pd.read_csv("data/IMOEX_filled.csv").tail(100)
    preds = info["predict_imoex"]
    last_hundred = pd.concat([df_filled, preds], ignore_index=True)
    last_hundred.to_csv('data/last_hundred_imoex.csv', index = False)

    s3_hook.load_file("data/last_hundred_imoex.csv", key = "last_hundred_imoex.csv", 
                        bucket_name="studcamp-ml", replace=True)
    remove('data/IMOEX_filled.csv')
    remove('data/last_hundred_imoex.csv')
    _LOG.info("Save last imoex finished.")
    info["save_last_imoex_end"] = datetime.now().strftime("%Y%m%d %H:%M")
    return info

def save_last_hundred_spbi(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="retrain_spbi")
    info["save_last_spbi_start"] = datetime.now().strftime("%Y%m%d %H:%M")   

    s3_hook.download_file(
        key = "SPBIRUS2_filled.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    df_filled = pd.read_csv("data/spbi_filled.csv").tail(100)
    preds = info["predict_spbi"]
    last_hundred = pd.concat([df_filled, preds], ignore_index=True)
    last_hundred.to_csv('data/last_hundred_spbi.csv', index = False)

    s3_hook.load_file("data/last_hundred_spbi.csv", key = "last_hundred_spbi.csv", 
                        bucket_name="studcamp-ml", replace=True)
    remove('data/SPBIRUS2_filled.csv')
    remove('data/last_hundred_spbi.csv')
    _LOG.info("Save last spbi finished.")
    info["save_last_spbi_end"] = datetime.now().strftime("%Y%m%d %H:%M")
    return info


t1 = PythonOperator(
    task_id="init",
    provide_context=True,
    python_callable=init,
    dag=dag
)

t2 = PythonOperator(
    task_id="scrape_news",
    provide_context=True,
    python_callable=scrape_news,
    dag=dag
)

t3 = PythonOperator(
    task_id="scrape_index",
    provide_context=True,
    python_callable=scrape_index,
    dag=dag
)

t4 = PythonOperator(
    task_id="query_forming_imoex",
    provide_context=True,
    python_callable=query_forming_imoex,
    dag=dag
)

t5 = PythonOperator(
    task_id = "query_forming_spbi",
    provide_context = True,
    python_callable = query_forming_spbi,
    dag = dag
)

t6 = PythonOperator(
    task_id="model_predict_imoex",
    provide_context=True,
    python_callable=model_predict_imoex,
    dag=dag
)

t7 = PythonOperator(
    task_id="retrain_imoex",
    provide_context=True,
    python_callable=retrain_model_imoex,
    dag=dag
)

t8 = PythonOperator(
    task_id="save_last_hundred_imoex",
    provide_context=True,
    python_callable=save_last_hundred_imoex,
    dag=dag
)

t9 = PythonOperator(
    task_id="model_predict_spbi",
    provide_context=True,
    python_callable=model_predict_spbi,
    dag=dag
)

t10 = PythonOperator(
    task_id="retrain_spbi",
    provide_context=True,
    python_callable=retrain_model_spbi,
    dag=dag
)

t11 = PythonOperator(
    task_id="save_last_hundred_spbi",
    provide_context=True,
    python_callable=save_last_hundred_spbi,
    dag=dag
)

t1 >> [t2, t3] >> [t4, t5]
t4 >> t6 >> t7 >> t8
t5 >> t9 >> t10 >> t11
    




