from datetime import timedelta
from datetime import datetime
import pandas as pd
from os import remove
from catboost import CatBoostRegressor as cbr
import joblib

from airflow import DAG
from airflow.operators.python import PythonOperator

from airflow.providers.amazon.aws.hooks.s3 import S3Hook

import logging
from news_parser import Scrapper
from airflow.utils.dates import days_ago
from botocore.exceptions import (
    ConnectTimeoutError,
    EndpointConnectionError,
    ConnectionError,
)
logger = logging.getLogger(__name__)
logger.addHandler(logging.StreamHandler())

cid = "s3_connection"
s3_hook = S3Hook(cid)


DEFAULT_ARGS = {
    "owner": "Team 22",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 3,
    "retry_delay": timedelta(minutes=1)
}

dag = DAG(
    "main_dag",
    tags=["mlops"],
    catchup=False,
    start_date=days_ago(2),
    default_args=DEFAULT_ARGS,
    schedule_interval="@once",
)

def init():
    """
    Step0: Pipeline initialisation.
    """
    info = {}
    info["start_timestamp"] = datetime.now().strftime("%Y%m%d %H:%M")
    # Импользуем данные с сегодня на 2 года назад.
    info["date_start"] = datetime.now().strftime("%Y-%m-%d")
    info["date_end"] = datetime.now().strftime("%Y-%m-%d")
    return info


def scrape_news(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="init")
    info["data_download_start"] = datetime.now().strftime("%Y%m%d %H:%M")
    start_date = info["date_start"]
    end_date = info["date_end"]

    scr = Scrapper("https://www.finam.ru/publications/section/market/date/", start_date, end_date)
    scr.parse()
    news_df = scr.estimate()

    s3_hook.download_file(
        key = "finam_news_scored.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    s3_news_df = pd.read_csv("finam_news_scored.csv")
    news_df = pd.concat([s3_news_df,news_df], ignore_index=True)
    news_df.to_csv('news.csv', index=False)
    s3_hook.load_file('news.csv', key = "finam_news_scored.csv", 
                          bucket_name="studcamp-ml", replace=True)
    remove('data/finam_news_scored.csv')
    
    _LOG.info("News loading finished.")

    info["data_download_end"] = datetime.now().strftime("%Y%m%d %H:%M")

    return info

def query_forming(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="scrape_news")
    info["query_forming_start"] = datetime.now().strftime("%Y%m%d %H:%M")

    max_news_count = 20
    lookback = 65

    s3_hook.download_file(
        key = "finam_news_scored.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    news = pd.read_csv("finam_news_scored.csv")

    s3_hook.download_file(
        key = "IMOEX_filled.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    df_filled = pd.read_csv("IMOEX_filled.csv")

    news["day"] = pd.to_datetime(news["day"])
    cur_date = news.iloc[-1].day

    query = pd.DataFrame()
    news_count = news[news.day == cur_date].shape[0]
    query["news_size"] = [news_count]
    query["year"] = [cur_date.year]
    query["month"] = [cur_date.month]
    query["day"] = [cur_date.day]
    remove('finam_news_scored.csv')
    remove('data/IMOEX_filled.csv')

    res = news[news.day == cur_date].sample(min(max_news_count, news_count)).score

    for i in range(max_news_count - min(max_news_count, news_count)):
        res = res.append(pd.Series([0.0]), ignore_index=True)

    res = res.reset_index(drop=True)

    for i in range(max_news_count):
        query[f"news{i}"] = res[i]

    lags = df_filled[df_filled.date.between(str(cur_date-timedelta(days=lookback)),str(cur_date))].open.reset_index(drop=True)
    
    for i in range(1,lookback+1):
        query["shift"+str(i)] = lags[lags.size-i]
    
    info["query"] = query
    _LOG.info("Query forming finished.")
    info["query_forming_end"] = datetime.now().strftime("%Y%m%d %H:%M")

    return info

def model_predict(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="query_forming")
    info["prediction_start"] = datetime.now().strftime("%Y%m%d %H:%M")

    s3_hook.download_file(
        key = "catboost.joblib",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    model = joblib.dump("catboost.joblib")
    query = info["query"]
    preds = model.predict(query)
    params = model.get_all_params()
    info["cb_params"] = params
    preds.to_csv('catboost_preds.csv', index = False)

    s3_hook.load_file('catboost_preds.csv', key = "prediction.csv", 
                        bucket_name="studcamp-ml", replace=True)
    remove('catboost.joblib')

    info["predict"] = preds
    _LOG.info("Prediction finished.")
    info["prediction_end"] = datetime.now().strftime("%Y%m%d %H:%M")
    return info
    
def save_last_hundred(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="prediction")
    info["save_last_start"] = datetime.now().strftime("%Y%m%d %H:%M")   

    s3_hook.download_file(
        key = "IMOEX_filled.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    df_filled = pd.read_csv("IMOEX_filled.csv").tail(100)
    preds = info["predict"]
    last_hundred = pd.concat([df_filled, preds], ignore_index=True)
    last_hundred.to_csv('last_hundred.csv', index = False)

    s3_hook.load_file(last_hundred, key = "last_hundred.csv", 
                        bucket_name="studcamp-ml", replace=True)
    remove('IMOEX_filled.csv')

    _LOG.info("Save finished.")
    info["save_last_end"] = datetime.now().strftime("%Y%m%d %H:%M")
    return info

def retrain_model(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="retraining")
    info["retraining_start"] = datetime.now().strftime("%Y%m%d %H:%M") 
    params = info["cb_params"]

    s3_hook.download_file(
        key = "for_catboost.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )

    df = pd.read_csv('for_catboost.csv')
    s3_hook.download_file(
        key = "yesterday_query.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )

    q = pd.read_csv('yesterday_query.csv')
    df = pd.concat([df, q], reset_index = False)
    cbr = cbr(**params)
    cbr.fit(df)
    model = joblib.dump("catboost.joblib")
    s3_hook.load_file(model, key = "catboost.joblib", 
                        bucket_name="studcamp-ml", replace=True)
    remove('catboost.joblib')

    _LOG.info("Retrain finished.")
    info["retraining_end"] = datetime.now().strftime("%Y%m%d %H:%M")
    return info

t1 = PythonOperator(
    task_id="init",
    provide_context=True,
    python_callable=init,
    dag=dag
)

t2 = PythonOperator(
    task_id="scrape_news",
    provide_context=True,
    python_callable=scrape_news,
    dag=dag
)

t3 = PythonOperator(
    task_id="MCHOMAK's code",
    provide_context=True,
    python_callable=blablabla,
    dag=dag
)

t4 = PythonOperator(
    task_id="query_forming",
    provide_context=True,
    python_callable=query_forming,
    dag=dag
)

t5 = PythonOperator(
    task_id="model_predict",
    provide_context=True,
    python_callable=model_predict,
    dag=dag
)

t6 = PythonOperator(
    task_id="save_last_hundred",
    provide_context=True,
    python_callable=save_last_hundred,
    dag=dag
)

t7 = PythonOperator(
    task_id="retraining",
    provide_context=True,
    python_callable=retrain_model,
    dag=dag
)

t1 >> [t2, t3] >> t4 >> t5 >> t6 >> t7
    




